// https://protobuf.dev/
syntax = "proto3";

service Inference {
  // todo: ref - https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto
  rpc SendRequest (Request) returns (Reply) {}
  rpc SendBatch (Batch) returns (Reply) {}
}

// abandoned
message Batch {
  uint32 size = 1;
  repeated Request batch = 2;
}

// definition of different message:
// Connect: type=0 | ['msg' = 1|0 (connect|disconnect)]
//                 | ['stat'] ['batch_size'] ['slo']
//                 | ['id'] ['model_name'] ['logdir']
//                 | ['client_addr'] ['frontend_addr'] (['next_frontend'])
// Request: type=1 | rid sid sub addr ['slo'] ['batch_size'] ['batch_id']
// Result:  type=2 | rid sid sub 'done' 'res' latencies
// Message: type=3 | ['stat'] ['batch_size']
// Message: type=4 | scaling_factor
// Heartbeat: type=5 | ['stat'] ['qsize'] ['proc_rate']

// C->Client; F->Frontend; W->Worker
enum Type {
  // unknown: default value
  TYPE_UNKNOWN = 0;
  // connect: the first request to make a connection
  // [C->F]: info{msg=1|0, slo, init_qps}
  //         msg{logdir, model_name, client_addr, }
  //         bytes (-> dummy request)
  //         scaling_factor:
  //             # fixme: add number of frontend & workers
  //             # all_frontend_addr_list? or in config?
  // [F->W]: info{msg=1|0, slo, module_slo, stat, batch_size}
  //         msg{id, logdir, model_name, client_addr, frontend_addr, [next_frontend]}
  //         bytes (-> dummy request)
  // [F->F]:
  TYPE_CONNECT = 1;
  TYPE_REQUEST = 2;   // request: rid, sid, sub, addr, time, bytes, info{batch_size, last_in_batch}
  TYPE_RESULT = 3;    // result:  rid, sid, sub, addr, time, info{res, done}
  TYPE_CONTROL = 4;   // control [F->W]: info{[batch_size|stat]}  (stat: on, off, idle->warmup)
  // control [W->F]: info{id, stat}  (stat: on->launched, off->shutdown)
  TYPE_MESSAGE = 5;   // message: [F->F]: unused
  TYPE_HEARTBEAT = 6; // heartbeat [W->F]: info{id, stat, qsize, avg_lat, proc_rate, batch_size}
  TYPE_DUMMY = 7;     // dummy request: bytes
  TYPE_SCALING = 8;   // scaling: scaling_factor (rate_future)
}

message Request{
  Type type = 1;
  uint32 rid = 2; // start from 0
  repeated uint32 sub = 3; // number of sub-request
  string addr = 4; // address of client (ip:port)

  // Request: [slo | batch_size | batch_id]
  // Control: [stat | batch_size | scaling_factor]
  // usage: if xxx in info.keys(): ...
  // usage: data = dict(info) ...
  map<string, uint32> info = 10;
  map<string, string> msg = 11; // additional message (next frontend addr...)
  float scaling_factor = 12; // 0.0 -> single module | >0 -> double module

  bytes bytes = 15; // request content to be inferred (img, text, etc.)
  repeated string res = 16; // inference results

  Timestamp begin = 20;
  // The latency of the request from client to each Frontend and Worker (arriving)
  // [frontend-1, worker-1-1, frontend-2, worker-2-1, ... ]
  repeated float latency = 21;
}

message Reply{
  // flag = 0 | 1 (OK | ERROR)
  uint32 flag = 1;
  uint32 qsize = 2;  // [W->F]
}

message Timestamp {
  int64 seconds = 1;
  int32 microseconds = 2;
}
